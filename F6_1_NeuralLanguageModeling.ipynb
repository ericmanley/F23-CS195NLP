{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "  <div>\n",
    "      <h1>CS 195: Natural Language Processing</h1>\n",
    "      <h2>Neural Language Modeling</h2>\n",
    "      </br>\n",
    "    <a href=\"https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_1_NeuralLanguageModeling.ipynb\">\n",
    "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
    "    </a>\n",
    "  </div>\n",
    "  <div style=\"margin-left: 20px;\">\n",
    "    <img src=\"images/dalle_neural_net_viz.png\" width=\"500\" style=\"display: block;\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cover Illustration:** generated by Dall E using the ChatGPT 4 interface, prompted for a visualization of the network used in the code below. *That's not what I meant.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Announcement\n",
    "\n",
    "AI - English Faculty Candidate: Gabriel Ford\n",
    "\n",
    "Meeting with students: Thursday at 3:30pm in Howard 309\n",
    "\n",
    "Scholarly Presentation: Friday at 9:00am in Howard ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "SLP: Neural Networks and Neural Language Models, Chapter 7 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset for today\n",
    "\n",
    "AG News dataset\n",
    "* short news articles\n",
    "* four classes: World, Sports, Business, Sci/Tech\n",
    "\n",
    "https://huggingface.co/datasets/ag_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "print(data[\"train\"][\"text\"][0])\n",
    "\n",
    "# 0 is World\n",
    "# 1 is Sports\n",
    "# 2 is Business\n",
    "# 3 is Sci/Tech\n",
    "print(data[\"train\"][\"label\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Text Classification with Keras\n",
    "\n",
    "Last time, we saw \n",
    "* how to do text classification when there are more than 2 classes\n",
    "    - one hot encoded output layer, one node per class, *softmax* output\n",
    "    - categorical crossentropy loss\n",
    "* embedding layer\n",
    "    - pad sequences to all be same length\n",
    "    - learn vector for each word representing word semantics\n",
    "    \n",
    "<div>\n",
    "    <img src=\"images/neural_text_classification.png\">\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 7.11, https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "\n",
    "*pooling* combines/aggregates all of the embeddings in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.3325 - accuracy: 0.8764\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.1108 - accuracy: 0.9634\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0297 - accuracy: 0.9916\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0161 - accuracy: 0.9963\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0107 - accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 28s 8ms/step - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0039 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0033 - accuracy: 0.9986\n",
      "238/238 [==============================] - 0s 570us/step - loss: 0.6997 - accuracy: 0.8991\n",
      "Test accuracy: 89.91%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
    "\n",
    "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
    "max_length = 100  # This should be adjusted based on the dataset\n",
    "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "#create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "#use one of these instead of Flatten() to try a pooling method\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Language Modeling\n",
    "\n",
    "**Neural Language Modeling:** predict next word(s) from previous ones - like what we did with Markov models\n",
    "\n",
    "Like classification, but output is softmax of every possible word in the vocabulary\n",
    "\n",
    "Often a first step before extending the model to do summarization, translation, dialog, and other NLP tasks\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/neural_language_modeling.png\">\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 7.13, https://web.stanford.edu/~jurafsky/slp3/7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Neural Language Model in Keras\n",
    "\n",
    "Let's start by sampling some data from our news dataset\n",
    "\n",
    "Then split into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18682\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "data_subset, _ = train_test_split(data[\"train\"][\"text\"],train_size=5000)\n",
    "train_data, test_data = train_test_split(data_subset,train_size=0.8)\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_subset)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\",vocabulary_size)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "train_texts = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing training examples\n",
    "\n",
    "We want to take the sequences like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2378, 4322, 428, 1, 2378, 1138, 1345, 1325, 2354, 71, 35, 3, 625, 1768, 8, 1, 9177, 5819, 1877, 4619, 21, 4755, 3996]\n"
     ]
    }
   ],
   "source": [
    "print( train_texts[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and slide a window across to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use [2378, 4322, 428, 1, 2378] to predict 1138\n",
      "Use [4322, 428, 1, 2378, 1138] to predict 1345\n",
      "Use [428, 1, 2378, 1138, 1345] to predict 1325\n",
      "Use [1, 2378, 1138, 1345, 1325] to predict 2354\n",
      "etc.\n"
     ]
    }
   ],
   "source": [
    "print(\"Use\",train_texts[0][0:5],\"to predict\",train_texts[0][5])\n",
    "print(\"Use\",train_texts[0][1:6],\"to predict\",train_texts[0][6])\n",
    "print(\"Use\",train_texts[0][2:7],\"to predict\",train_texts[0][7])\n",
    "print(\"Use\",train_texts[0][3:8],\"to predict\",train_texts[0][8])\n",
    "print(\"etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Group Discussion\n",
    "\n",
    "What data structures (lists, matrixes, etc.) do we need to prepare to make this a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing all of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 138859\n",
      "First train text: [2378, 4322, 428, 1, 2378, 1138, 1345, 1325, 2354, 71, 35, 3, 625, 1768, 8, 1, 9177, 5819, 1877, 4619, 21, 4755, 3996]\n",
      "Example sequence 0: [2378, 4322, 428, 1, 2378]  target: 1138\n",
      "Example sequence 1: [4322, 428, 1, 2378, 1138]  target: 1345\n",
      "Example sequence 2: [428, 1, 2378, 1138, 1345]  target: 1325\n",
      "Example sequence 3: [1, 2378, 1138, 1345, 1325]  target: 2354\n",
      "Example sequence 4: [2378, 1138, 1345, 1325, 2354]  target: 71\n",
      "Example sequence 5: [1138, 1345, 1325, 2354, 71]  target: 35\n"
     ]
    }
   ],
   "source": [
    "# Decide the sequence length\n",
    "sequence_length = 5  # Length of the input sequence before predicting the next word\n",
    "\n",
    "# Create the sequences\n",
    "predictor_sequences = []\n",
    "targets = []\n",
    "for text in train_texts:\n",
    "    for i in range(sequence_length, len(text)):\n",
    "        # Take the sequence of tokens as input and the next token as target\n",
    "        curr_target = text[i]\n",
    "        curr_predictor_sequence = text[i-sequence_length:i]\n",
    "        predictor_sequences.append(curr_predictor_sequence)\n",
    "        targets.append(curr_target)\n",
    "\n",
    "        \n",
    "print(\"Number of sequences:\",len(predictor_sequences))\n",
    "\n",
    "\n",
    "print(\"First train text:\",train_texts[0])\n",
    "print(\"Example sequence 0:\",predictor_sequences[0],\" target:\",targets[0])\n",
    "print(\"Example sequence 1:\",predictor_sequences[1],\" target:\",targets[1])\n",
    "print(\"Example sequence 2:\",predictor_sequences[2],\" target:\",targets[2])\n",
    "print(\"Example sequence 3:\",predictor_sequences[3],\" target:\",targets[3])\n",
    "print(\"Example sequence 4:\",predictor_sequences[4],\" target:\",targets[4])\n",
    "print(\"Example sequence 5:\",predictor_sequences[5],\" target:\",targets[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding\n",
    "\n",
    "Some of the sequences might be really short - so we'll pad them just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform length\n",
    "predictor_sequences_padded = pad_sequences(predictor_sequences, maxlen=sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The target output\n",
    "\n",
    "Since we're making this into a classification problem, the output layer needs to have one node for each word in the vocabulary. \n",
    "\n",
    "Target values need to be transformed into a one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors words 0: [2378 4322  428    1 2378]\n",
      "target word 0: 1138\n",
      "target word 0 one hot encoded: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert output to one-hot encoding\n",
    "target_word_one_hot = to_categorical(targets, num_classes=vocabulary_size)\n",
    "\n",
    "print(\"Predictors words 0:\", predictor_sequences_padded[0])\n",
    "print(\"target word 0:\", targets[0])\n",
    "print(\"target word 0 one hot encoded:\", target_word_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing the test set\n",
    "\n",
    "We have to do all of those same things for the test set.\n",
    "\n",
    "**Group Exercise:** Turn this into a function so that you can use it to prepare both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Create the sequences\n",
    "predictor_sequences_test = []\n",
    "targets_test = []\n",
    "for text in test_texts:\n",
    "    for i in range(sequence_length, len(text)):\n",
    "        # Take the sequence of tokens as input and the next token as target\n",
    "        curr_target = text[i]\n",
    "        curr_predictor_sequence = text[i-sequence_length:i]\n",
    "        predictor_sequences_test.append(curr_predictor_sequence)\n",
    "        targets_test.append(curr_target)\n",
    "        \n",
    "# Pad sequences to ensure uniform length\n",
    "predictor_sequences_padded_test = pad_sequences(predictor_sequences_test, maxlen=sequence_length, padding='pre')\n",
    "\n",
    "# Convert target to one-hot encoding\n",
    "target_word_one_hot_test = to_categorical(targets_test, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Designing the Neural Network\n",
    "\n",
    "We'll start with a simple network like the one we used for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4340/4340 [==============================] - 36s 8ms/step - loss: 7.5818 - accuracy: 0.0720 - val_loss: 7.2768 - val_accuracy: 0.0975\n",
      "Epoch 2/10\n",
      "4340/4340 [==============================] - 36s 8ms/step - loss: 6.6734 - accuracy: 0.1141 - val_loss: 7.1562 - val_accuracy: 0.1233\n",
      "Epoch 3/10\n",
      "4340/4340 [==============================] - 35s 8ms/step - loss: 6.0304 - accuracy: 0.1458 - val_loss: 7.3224 - val_accuracy: 0.1351\n",
      "Epoch 4/10\n",
      "4340/4340 [==============================] - 37s 8ms/step - loss: 5.4349 - accuracy: 0.1756 - val_loss: 7.7051 - val_accuracy: 0.1417\n",
      "Epoch 5/10\n",
      "4340/4340 [==============================] - 37s 8ms/step - loss: 4.8688 - accuracy: 0.2079 - val_loss: 8.3195 - val_accuracy: 0.1419\n",
      "Epoch 6/10\n",
      "4340/4340 [==============================] - 37s 8ms/step - loss: 4.3260 - accuracy: 0.2433 - val_loss: 9.1284 - val_accuracy: 0.1377\n",
      "Epoch 7/10\n",
      "4340/4340 [==============================] - 36s 8ms/step - loss: 3.8050 - accuracy: 0.2890 - val_loss: 10.0640 - val_accuracy: 0.1294\n",
      "Epoch 8/10\n",
      "4340/4340 [==============================] - 36s 8ms/step - loss: 3.3213 - accuracy: 0.3473 - val_loss: 11.0950 - val_accuracy: 0.1243\n",
      "Epoch 9/10\n",
      "4340/4340 [==============================] - 37s 8ms/step - loss: 2.8924 - accuracy: 0.4141 - val_loss: 12.1845 - val_accuracy: 0.1168\n",
      "Epoch 10/10\n",
      "4340/4340 [==============================] - 36s 8ms/step - loss: 2.5328 - accuracy: 0.4784 - val_loss: 13.2289 - val_accuracy: 0.1154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x54b860940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=sequence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model - you can also pass in the test set\n",
    "model.fit(predictor_sequences_padded, target_word_one_hot, epochs=10, verbose=1, validation_data=(predictor_sequences_padded_test, target_word_one_hot_test))\n",
    "\n",
    "# The model can now be used to predict the next word in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing the final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062/1062 [==============================] - 4s 3ms/step - loss: 13.2289 - accuracy: 0.1154\n",
      "Test accuracy: 11.54%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(predictor_sequences_padded_test, target_word_one_hot_test)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "We can use this model to successively generate words based on previous ones - like our Markov sequence on steroids.\n",
    "\n",
    "Let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 84, 19, 11, 18]]\n",
      "[[ 1 84 19 11 18]]\n"
     ]
    }
   ],
   "source": [
    "starter_string = \"the government said that it\"\n",
    "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
    "print(tokens_list)\n",
    "\n",
    "tokens_array = np.array(tokens_list)\n",
    "print(tokens_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model will predict probabilities for each possible word in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.7898148e-12 6.4446380e-05 6.8999807e-06 ... 7.7608224e-12\n",
      "  7.1798600e-12 7.7865622e-12]]\n",
      "We get a probability for each of the 18682 words\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = model.predict(tokens_array,verbose=0)\n",
    "print(predicted_probabilities)\n",
    "print(\"We get a probability for each of the\",len(predicted_probabilities[0]),\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we could get the word associated with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index: 99\n",
      "word: would\n"
     ]
    }
   ],
   "source": [
    "predicted_index = np.argmax(predicted_probabilities)\n",
    "print(\"word index:\",predicted_index)\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "print(\"word:\",predicted_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you could generate a random word according the these probabilities (like with did with Markov text generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would introduce a definitive jury in the 12th season now the surface of a bloody suspension mine in georgia that it will keep him to proceed for the sky oil assets to 2006 what might offer an initial four time in the united states towards the new braves on wednesday "
     ]
    }
   ],
   "source": [
    "starter_string = \"the government said that it\"\n",
    "tokens_list = tokenizer.texts_to_sequences([starter_string])\n",
    "tokens = tokens_list[0]\n",
    "\n",
    "for i in range(50):\n",
    "    curr_seq = tokens[-sequence_length:]\n",
    "    curr_array = np.array([curr_seq])\n",
    "    predicted_probabilities = model.predict(curr_array,verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probabilities)\n",
    "    predicted_word = tokenizer.index_word[predicted_index]\n",
    "    print(predicted_word+\" \",end=\"\")\n",
    "    tokens.append(predicted_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Pick another dataset and get it working with this code\n",
    "* you will likely need to prepare the text a little differently - do you need to first break it into sentences?\n",
    "* describe your dataset and what you did to prepare it\n",
    "\n",
    "Perform a neural language modeling experiment\n",
    "* experiment with something to try to find a well-performing model\n",
    "    * sliding window size\n",
    "    * number of hidden nodes in the network\n",
    "    * learning rate\n",
    "* describe what you did and write up an interpretation of your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "#an example on changing the learning rate\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
