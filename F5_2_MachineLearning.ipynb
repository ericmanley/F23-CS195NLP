{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Machine Learning with Text Data\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F5_2_MachineLearning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "SLP: Vector Semantics and Embeddings, Chapter 6 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "scikit-learn API reference: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.4)\n",
      "Requirement already satisfied: sklearn in /Users/000794593/Library/Python/3.10/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: keras in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from sklearn) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow) (63.4.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.41.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets sklearn keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: A typical machine learning setup\n",
    "\n",
    "Source: https://github.com/merriekay/S23-CS167-Notes/blob/main/Day11_Scikit_Learn_Practice.ipynb\n",
    "\n",
    "**Discussion Question:** What are `train_data` and `train_sln` in `dt.fit(train_data,train_sln)`?\n",
    "* What shapes do they have?\n",
    "* What data types are involved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "___PREDICTED___ \t  ___ACTUAL___\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-virginica \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-virginica \t\t Iris-virginica\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "Iris-setosa \t\t Iris-setosa\n",
      "Iris-versicolor \t\t Iris-versicolor\n",
      "-------------------------------------------------------\n",
      "                 Iris-setosa  Iris-versicolor  Iris-virginica\n",
      "Iris-setosa                9                0               0\n",
      "Iris-versicolor            0               10               1\n",
      "Iris-virginica             0                2               8\n"
     ]
    }
   ],
   "source": [
    "#classic scikit-learn algorithm\n",
    "\n",
    "#0. import libraries\n",
    "import sklearn\n",
    "import pandas\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "\n",
    "#1. load data\n",
    "#path = 'datasets/irisData.csv' #'/content/drive/MyDrive/CS167/datasets/irisData.csv'\n",
    "path = \"https://raw.githubusercontent.com/ericmanley/F23-CS195NLP/main/data/irisData.csv\"\n",
    "iris_data = pandas.read_csv(path)\n",
    "\n",
    "#2. split data\n",
    "predictors = ['sepal length', 'sepal width','petal length', 'petal width']\n",
    "target = \"species\"\n",
    "train_data, test_data, train_sln, test_sln = \\\n",
    "        train_test_split(iris_data[predictors], iris_data[target], test_size = 0.2, random_state=41)\n",
    "\n",
    "#3. Create classifier/regressor object (change these parameters for Exercise #1)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "#4. Call fit (to train the classification/regression model)\n",
    "dt.fit(train_data,train_sln)\n",
    "\n",
    "#5. Call predict to generate predictions\n",
    "iris_predictions = dt.predict(test_data)\n",
    "\n",
    "#6. Call a metric function to measure performance\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test_sln,iris_predictions))\n",
    "\n",
    "# Show the acutal and predicted (this isn't necessary, but may help catch bugs)\n",
    "print(\"___PREDICTED___ \\t  ___ACTUAL___\")\n",
    "for i in range(len(test_sln)):\n",
    "    print(iris_predictions[i],\"\\t\\t\", test_sln.iloc[i])\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "#print out a confusion matrix\n",
    "iris_labels= [\"Iris-setosa\", \"Iris-versicolor\",\"Iris-virginica\"]\n",
    "conf_mat = metrics.confusion_matrix(test_sln, iris_predictions, labels=iris_labels)\n",
    "print(pandas.DataFrame(conf_mat,index = iris_labels, columns = iris_labels))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making it work for text data\n",
    "\n",
    "To prepare your text data, you'll need to\n",
    "* tokenize\n",
    "* encode (get a numerical representation)\n",
    "\n",
    "Let's practice preparing a Hugging Face dataset for spam detection.\n",
    "\n",
    "https://huggingface.co/datasets/Deysi/spam-detection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8175\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get rich quick!!! Make $$$ in just 30 minutes, GUARANTEED! No skills or experience required! Just send me a small fee and I'll give you access to my exclusive money-making secrets!\n",
      "\n",
      "\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][32] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][32] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time poster in this sub, and I'm only an amateur with data compared to many.\n",
      "\n",
      "But, want I want to do is figure out the partisan lean for each of the new state house districts in Texas. This type of analysis is easily available for the federal districts at sites like 538 ( https://projects.fivethirtyeight.com/redistricting-2022-maps ), but I want to do it at the state house level.\n",
      "\n",
      "Then, as a second step, it'd be great to have the correct data to draw my own state congressional districts and derive partisan leans (as well as determine the racial makeup). But, this would be a much larger project for the future.\n",
      "\n",
      "Anyway, I'm struggling to figure out where to even start. I could get precinct data from the MIT data lab, but state districts break up even precincts, so they wouldn't be totally accurate.\n",
      "\n",
      "Thanks in advance for any thoughts or help!\n",
      "not_spam\n"
     ]
    }
   ],
   "source": [
    "print( data[\"train\"][\"text\"][35] )\n",
    "\n",
    "print( data[\"train\"][\"label\"][35] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "This could be done with any of the tokenization methods we've looked at.\n",
    "\n",
    "Let's try it with the WordPiece algorithm (used by the BERT models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'poster', 'in', 'this', 'sub', ',', 'and', 'i', \"'\", 'm', 'only', 'an', 'amateur', 'with', 'data', 'compared', 'to', 'many', '.', 'but', ',', 'want', 'i', 'want', 'to', 'do', 'is', 'figure', 'out', 'the', 'partisan', 'lean', 'for', 'each', 'of', 'the', 'new', 'state', 'house', 'districts', 'in', 'texas', '.', 'this', 'type', 'of', 'analysis', 'is', 'easily', 'available', 'for', 'the', 'federal', 'districts', 'at', 'sites', 'like', '53', '##8', '(', 'https', ':', '/', '/', 'projects', '.', 'five', '##thi', '##rt', '##ye', '##ight', '.', 'com', '/', 'red', '##ist', '##ricting', '-', '202', '##2', '-', 'maps', ')', ',', 'but', 'i', 'want', 'to', 'do', 'it', 'at', 'the', 'state', 'house', 'level', '.', 'then', ',', 'as', 'a', 'second', 'step', ',', 'it', \"'\", 'd', 'be', 'great', 'to', 'have', 'the', 'correct', 'data', 'to', 'draw', 'my', 'own', 'state', 'congressional', 'districts', 'and', 'derive', 'partisan', 'leans', '(', 'as', 'well', 'as', 'determine', 'the', 'racial', 'makeup', ')', '.', 'but', ',', 'this', 'would', 'be', 'a', 'much', 'larger', 'project', 'for', 'the', 'future', '.', 'anyway', ',', 'i', \"'\", 'm', 'struggling', 'to', 'figure', 'out', 'where', 'to', 'even', 'start', '.', 'i', 'could', 'get', 'precinct', 'data', 'from', 'the', 'mit', 'data', 'lab', ',', 'but', 'state', 'districts', 'break', 'up', 'even', 'precinct', '##s', ',', 'so', 'they', 'wouldn', \"'\", 't', 'be', 'totally', 'accurate', '.', 'thanks', 'in', 'advance', 'for', 'any', 'thoughts', 'or', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer.tokenize(data[\"train\"][\"text\"][35])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Integer Encodings\n",
    "\n",
    "The Hugging Face tokenizers assign a numerical ID to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2034,\n",
       " 2051,\n",
       " 13082,\n",
       " 1999,\n",
       " 2023,\n",
       " 4942,\n",
       " 1010,\n",
       " 1998,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 2069,\n",
       " 2019,\n",
       " 5515,\n",
       " 2007,\n",
       " 2951,\n",
       " 4102,\n",
       " 2000,\n",
       " 2116,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2215,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2003,\n",
       " 3275,\n",
       " 2041,\n",
       " 1996,\n",
       " 14254,\n",
       " 8155,\n",
       " 2005,\n",
       " 2169,\n",
       " 1997,\n",
       " 1996,\n",
       " 2047,\n",
       " 2110,\n",
       " 2160,\n",
       " 4733,\n",
       " 1999,\n",
       " 3146,\n",
       " 1012,\n",
       " 2023,\n",
       " 2828,\n",
       " 1997,\n",
       " 4106,\n",
       " 2003,\n",
       " 4089,\n",
       " 2800,\n",
       " 2005,\n",
       " 1996,\n",
       " 2976,\n",
       " 4733,\n",
       " 2012,\n",
       " 4573,\n",
       " 2066,\n",
       " 5187,\n",
       " 2620,\n",
       " 1006,\n",
       " 16770,\n",
       " 1024,\n",
       " 1013,\n",
       " 1013,\n",
       " 3934,\n",
       " 1012,\n",
       " 2274,\n",
       " 15222,\n",
       " 5339,\n",
       " 6672,\n",
       " 18743,\n",
       " 1012,\n",
       " 4012,\n",
       " 1013,\n",
       " 2417,\n",
       " 2923,\n",
       " 28827,\n",
       " 1011,\n",
       " 16798,\n",
       " 2475,\n",
       " 1011,\n",
       " 7341,\n",
       " 1007,\n",
       " 1010,\n",
       " 2021,\n",
       " 1045,\n",
       " 2215,\n",
       " 2000,\n",
       " 2079,\n",
       " 2009,\n",
       " 2012,\n",
       " 1996,\n",
       " 2110,\n",
       " 2160,\n",
       " 2504,\n",
       " 1012,\n",
       " 2059,\n",
       " 1010,\n",
       " 2004,\n",
       " 1037,\n",
       " 2117,\n",
       " 3357,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1040,\n",
       " 2022,\n",
       " 2307,\n",
       " 2000,\n",
       " 2031,\n",
       " 1996,\n",
       " 6149,\n",
       " 2951,\n",
       " 2000,\n",
       " 4009,\n",
       " 2026,\n",
       " 2219,\n",
       " 2110,\n",
       " 7740,\n",
       " 4733,\n",
       " 1998,\n",
       " 18547,\n",
       " 14254,\n",
       " 12671,\n",
       " 1006,\n",
       " 2004,\n",
       " 2092,\n",
       " 2004,\n",
       " 5646,\n",
       " 1996,\n",
       " 5762,\n",
       " 5789,\n",
       " 1007,\n",
       " 1012,\n",
       " 2021,\n",
       " 1010,\n",
       " 2023,\n",
       " 2052,\n",
       " 2022,\n",
       " 1037,\n",
       " 2172,\n",
       " 3469,\n",
       " 2622,\n",
       " 2005,\n",
       " 1996,\n",
       " 2925,\n",
       " 1012,\n",
       " 4312,\n",
       " 1010,\n",
       " 1045,\n",
       " 1005,\n",
       " 1049,\n",
       " 8084,\n",
       " 2000,\n",
       " 3275,\n",
       " 2041,\n",
       " 2073,\n",
       " 2000,\n",
       " 2130,\n",
       " 2707,\n",
       " 1012,\n",
       " 1045,\n",
       " 2071,\n",
       " 2131,\n",
       " 18761,\n",
       " 2951,\n",
       " 2013,\n",
       " 1996,\n",
       " 10210,\n",
       " 2951,\n",
       " 6845,\n",
       " 1010,\n",
       " 2021,\n",
       " 2110,\n",
       " 4733,\n",
       " 3338,\n",
       " 2039,\n",
       " 2130,\n",
       " 18761,\n",
       " 2015,\n",
       " 1010,\n",
       " 2061,\n",
       " 2027,\n",
       " 2876,\n",
       " 1005,\n",
       " 1056,\n",
       " 2022,\n",
       " 6135,\n",
       " 8321,\n",
       " 1012,\n",
       " 4283,\n",
       " 1999,\n",
       " 5083,\n",
       " 2005,\n",
       " 2151,\n",
       " 4301,\n",
       " 2030,\n",
       " 2393,\n",
       " 999]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get the encodings for all the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2131, 4138, 4248, 999, 999, 999, 2191, 1002, 1002, 1002, 1999, 2074, 2382, 2781, 1010, 12361, 999, 2053, 4813, 2030, 3325, 3223, 999, 2074, 4604, 2033, 1037, 2235, 7408, 1998, 1045, 1005, 2222, 2507, 2017, 3229, 2000, 2026, 7262, 2769, 1011, 2437, 7800, 999], [2131, 4138, 4248, 1998, 3733, 2007, 16021, 2696, 15671, 2232, 999, 999, 2024, 2017, 5458, 1997, 2108, 3631, 1998, 2542, 3477, 5403, 3600, 2000, 3477, 5403, 3600, 1029, 2079, 2017, 2215, 2000, 2191, 2070, 3733, 2769, 2302, 5128, 1999, 2151, 2613, 3947, 1029, 2092, 1010, 2298, 2053, 2582, 2138, 16021, 2696, 15671, 2232, 2038, 2288, 2017, 3139, 999, 2007, 2256, 6208, 2291, 1010, 2017, 2064, 2191, 2039, 2000, 1002, 6694, 1037, 2154, 2074, 2011, 16663, 1010, 6631, 1010, 1998, 15591, 2006, 8466, 1012, 2009, 1005, 1055, 2008, 3722, 999, 2053, 4813, 2030, 3325, 4072, 1010, 2074, 1037, 26381, 1998, 2019, 4274, 4434, 1012, 1998, 1996, 2190, 2112, 2003, 1010, 2017, 2064, 2707, 7414, 3202, 1012, 2053, 3403, 2005, 6226, 2015, 2030, 8552, 3696, 1011, 2039, 6194, 1012, 2074, 8816, 1996, 16021, 2696, 15671, 2232, 10439, 1998], [4931, 2045, 999, 2024, 2017, 5458, 1997, 3110, 2894, 1998, 23657, 1029, 2079, 2017, 2215, 2000, 7532, 2007, 2500, 2040, 3745, 2115, 5426, 1998, 25289, 1029, 2298, 2053, 2582, 2084, 2256, 6429, 2591, 2897, 999, 2256, 4132, 2003, 1996, 3819, 2173, 2000, 12403, 2213, 2115, 2814, 2007, 7696, 1998, 14389, 1010, 2191, 8275, 6115, 1010, 1998, 3745, 2659, 1011, 3737, 2033, 7834, 1012, 2057, 2031, 5717, 4781, 2005, 4180, 3737, 2030, 5310, 5248, 1010, 2061, 2017, 2064, 2514, 2489, 2000, 2022, 2004, 27885, 3630, 25171, 1998, 23217, 3512, 2004, 2017, 2215, 1012, 4606, 1010, 2057, 3749, 6197, 1997, 2838, 2008, 2017, 1005, 2222, 2196, 2224, 1010, 2107, 2004, 23100, 19461, 11254, 1010, 11809, 14592, 1010, 1998, 15703, 26828, 2015, 2008, 2017, 2064, 1005, 1056, 2735, 2125, 1012, 2017, 1005, 2222, 2036, 2293, 2256, 5377, 10400, 1997, 14997]]\n"
     ]
    }
   ],
   "source": [
    "encodings = []\n",
    "for curr_example in data[\"train\"][\"text\"]:\n",
    "    curr_tokens = tokenizer.tokenize(curr_example)\n",
    "    curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "    encodings.append(curr_encodings)\n",
    "        \n",
    "#display some of them\n",
    "print(encodings[32:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the warning is telling you that the data set has token sequences that are too big for the BERT model - it'll still produce the correct tokens, though, and we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Discussion Question\n",
    "\n",
    "The hope is that we could now use the encodings as the predictors and fit a model to them like this. \n",
    "\n",
    "But this doesn't work - what is the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(encodings,data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726605504587156\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "\n",
    "def prepare_text_data(text_list,tokenizer,encoded_length=512):\n",
    "    encodings = []\n",
    "    for curr_example in text_list:\n",
    "        curr_tokens = tokenizer.tokenize(curr_example)\n",
    "        curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "        # truncate sequences that are too long\n",
    "        if len(curr_encodings) > encoded_length:\n",
    "            curr_encodings = curr_encodings[:encoded_length]\n",
    "        # pad sequences that are too short with 0s\n",
    "        elif len(curr_encodings) < encoded_length:\n",
    "            curr_encodings = curr_encodings + [0]*(encoded_length-len(curr_encodings))\n",
    "\n",
    "        encodings.append(curr_encodings)\n",
    "        \n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group Discussion\n",
    "\n",
    "Is this a good accuracy score?\n",
    "\n",
    "What context do we need to interpret this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Group Exercise\n",
    "\n",
    "Have everyone at your table agree on another classification model that you've used before. \n",
    "\n",
    "Run this code but with the other model. How do the results compare?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Exercise\n",
    "\n",
    "Pick a different *text classification* dataset and test the accuracy of this approach on that data. How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with this encoding\n",
    "\n",
    "The numerical encoding we've used may not work well in many situations.\n",
    "\n",
    "Think about the predictor features our model is using\n",
    "\n",
    "`['first', 'time', 'poster', 'in', 'this', 'sub',`\n",
    "\n",
    "`[2034, 2051, 13082, 1999, 2023, 4942,`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first feature is \"first token in the sequence\"\n",
    "\n",
    "2nd feature is \"second token in the sequence\"\n",
    "\n",
    "etc.\n",
    "\n",
    "If a word appears in different positions in two different sequences, the model won't inherently recognize that it's the same word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The way words get mapped to numbers is arbitrary - there's no relationship between words that have close numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2034]\n",
      "[2051]\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.encode(\"first\",add_special_tokens=False) )\n",
    "print( tokenizer.encode(\"time\",add_special_tokens=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words Encoding\n",
    "\n",
    "Choose vocabulary (say 5000 most common words) one column for each word\n",
    "\n",
    "row contains counts for each word\n",
    "\n",
    "**Example**\n",
    "\n",
    "*Sentence 1:* \"The cat sat on the hat\"\n",
    "\n",
    "*Sentence 2:* \"The dog ate the cat and the hat\" \n",
    "\n",
    "*Vocabulary:* { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "\n",
    "|            | the | cat | sat | on | hat | dog | ate | and |\n",
    "|------------|-----|-----|-----|----|-----|-----|-----|-----|\n",
    "| Sentence 1 | 2   | 1   | 1   | 1  | 1   | 0   | 0   | 0   |\n",
    "| Sentence 2 | 3   | 1   | 0   | 0  | 1   | 1   | 1   | 1   |\n",
    "\n",
    "\n",
    "**The downside:** this doesn't maintain any information about word order - thus the \"bag\" of words\n",
    "\n",
    "`scikit-learn` provides a Bag-of-Words encoder called `CountVectorizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "# remove stop words\n",
    "vectorizer = CountVectorizer(max_features=5000,stop_words=\"english\")  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TD-IDF Encoding\n",
    "\n",
    "**TF-IDF:** Term Frequency - Inverse Document Frequency\n",
    "\n",
    "**Term Frequency:** How often does the word appear in the example, like CountVectorizer\n",
    "* actually take the $\\log$ of it\n",
    "\n",
    "**Document Frequency:** What fraction of the *documents* (or *training-examples*) does the word appear in?\n",
    "\n",
    "**Inverse Document Frequency:** (number of documents) / (number of documents containing the word)\n",
    "* if a word is in only a few documents, you get a big number\n",
    "* if a word appears in lots of documents, you get a small number\n",
    "\n",
    "When encoding a new example, multiply the Term Frequency of the word in this example by the Inverse Document Frequency of the training set\n",
    "* gives higher weight to words that are differentiators\n",
    "* stop words should automatically be de-emphasized\n",
    "\n",
    "**Example:**\n",
    "Document collection: all of Shakespeare's plays\n",
    "\n",
    "The word `Romeo` appears 113 times but only in 1 document\n",
    "\n",
    "The word `action` appears 113 time but in 31 documents \n",
    "\n",
    "so Romeo will get a much higher weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Try the Integer Encoding, Count Vectorizer, and TD-IDF on a *new dataset*\n",
    "* use at least three different classification models\n",
    "\n",
    "Give a short write-up on the following\n",
    "* Describe your dataset, including the distribution of the target variable\n",
    "* Describe the results of the machine learning experiment\n",
    "* Interpret the results - why do you think each of the encodings and algorithms performed the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## if time....\n",
    "\n",
    "Here's some code for building a multilayer neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 0s 808us/step - loss: 0.2739 - accuracy: 0.9697\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 805us/step - loss: 0.0388 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 796us/step - loss: 0.0156 - accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 772us/step - loss: 0.0088 - accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 773us/step - loss: 0.0058 - accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 771us/step - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 765us/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 816us/step - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 830us/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 783us/step - loss: 0.0016 - accuracy: 0.9996\n",
      "86/86 [==============================] - 0s 535us/step - loss: 0.0052 - accuracy: 0.9985\n",
      "Test accuracy: 99.85%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# the sklearn vectors need to be converted to numpy arrays for this library\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays = test_vectors.toarray()\n",
    "# Convert string labels to binary labels\n",
    "train_labels_array = np.array([1 if label == \"spam\" else 0 for label in train_labels])\n",
    "test_labels_array = np.array([1 if label == \"spam\" else 0 for label in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vectors_arrays, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
