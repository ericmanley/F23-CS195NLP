{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Handling Long-Term Information in Recurrent Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_4_LongTermRecurrence.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "SLP: RNNs and LSTMs, Chapter 9 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "Wikipedia article on Gated Recurrent Unit: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "\n",
    "Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras by Jason Brownlee: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.4)\n",
      "Requirement already satisfied: keras in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: transformers in /Users/000794593/Library/Python/3.10/lib/python/site-packages (4.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow) (63.4.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: filelock in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Sentence\n",
    "\n",
    "*The flights the airline was cancelling were full*\n",
    "\n",
    "Suppose we have generated `The flights the airline`\n",
    "* `was` is a good next choice\n",
    "   - `airline` has context for `was` vs. `were`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have generated `The flights the airline was cancelling`\n",
    "* `was`/`were` depends on `flights`\n",
    "* much more distance information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Vanishing Gradient\n",
    "\n",
    "The *vanishing gradient* is a common problem in deep neural networks\n",
    "\n",
    "If weights are < 1, they will get smaller and smaller each node they have to pass through - causing them to have little or not effect\n",
    "\n",
    "Happens during training too - error/loss is propogated backwards through the network proportional to the weights on each edge\n",
    "* earlier edges in the network are left with little error to use in adjusting weights\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/vanishing_gradient.png\">\n",
    "</div>\n",
    "\n",
    "image source: https://www.researchgate.net/figure/A-visualization-of-the-vanishing-gradient-problem-using-the-architecture-depicted-in_fig8_277603865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks try to address the vanishing gradient through\n",
    "* removing unneeded information from the context\n",
    "* adding information likely needed later\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LSTM_node.png\", width=700>\n",
    "</div>\n",
    "\n",
    "image_source: SLP Fig. 19.3, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How does it do this?\n",
    "\n",
    "* Explicit **context layer** $c_t$\n",
    "* neural **gates** that control the flow of information through the layer\n",
    "    - $f$ - the **forget gate** - delete info from context that is no longer needed\n",
    "    - $g$ - basic extraction of info from previous hidden state\n",
    "    - $i$ - the **add gate** - select info to add to current context\n",
    "    - $o$ - the **output gate** - decide what info is needed for current hidden state\n",
    "    \n",
    "<div>\n",
    "    <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/hadamard_product.png\"></td><td style=\"text-align: left;\"><b>Hadamard product:</b> bitwise multiplication</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sigmoid.png\"></td><td style=\"text-align: left;\"><b>Sigmoid activation:</b> pushes everything to 0 or 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/tanh.png\"></td><td style=\"text-align: left;\"><b>Hyperbolic tangent activation:</b> pushes to 0 or 1, more like identity at the origin</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "Combining sigmoid with ⊙ has the effect of *masking* out information removing some, leaving others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated Recurrent Unit\n",
    "\n",
    "A **Gated Recurrent Unit** is a popular unit similar to LSTM, except more lightweight\n",
    "* no output gate\n",
    "* no context vector\n",
    "\n",
    "Performance is often similar, but fewer parameters\n",
    "* faster\n",
    "* less memory\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/RNN-vs-LSTM-vs-GRU.png\", width=700>\n",
    "</div>\n",
    "\n",
    "image source: http://dprogrammer.org/rnn-lstm-gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's work with some data\n",
    "\n",
    "We'll do something that should be an easier learning problem: text classification with a recurrent network\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/RNN_classification.png\" width=700>\n",
    "</div>\n",
    "\n",
    "image source: SLP Fig. 9.8, https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment these to work with a subset of the data\n",
    "#data_subset_text, _, data_subset_label, _ = train_test_split(dataset[\"train\"][\"text\"],dataset[\"train\"][\"label\"],train_size=5000) \n",
    "#train_data_text,  test_data_text, train_data_label, test_data_label = train_test_split(data_subset_text, data_subset_label,test_size = 0.2)\n",
    "\n",
    "# uncomment these to use the full original data\n",
    "train_data_text = dataset[\"train\"][\"text\"]\n",
    "train_data_label = dataset[\"train\"][\"label\"]\n",
    "test_data_text = dataset[\"test\"][\"text\"]\n",
    "test_data_label = dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the capsule comment said \"great acting.\" In my opinion, these are two great actors giving horrible performances, and with zero chemistry with one another, for a great director in his all-time worst effort. Robert De Niro has to be the most ingenious and insightful illiterate of all time. Jane Fonda's performance uncomfortably drifts all over the map as she clearly has no handle on this character, mostly because the character is so poorly written. Molasses-like would be too swift an adjective for this film's excruciating pacing. Although the film's intent is to be an uplifting story of curing illiteracy, watching it is a true \"bummer.\" I give it 1 out of 10, truly one of the worst 20 movies for its budget level that I have ever seen.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#printing out a sample review\n",
    "print( train_data_text[125] )\n",
    "print( train_data_label[125] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SimpleRNN, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "pad_length = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) #only keep the 10000 most common words\n",
    "tokenizer.fit_on_texts(train_data_text)\n",
    "tokenized_train_data = tokenizer.texts_to_sequences(train_data_text)\n",
    "processed_train_data = pad_sequences(tokenized_train_data,maxlen=pad_length, padding='pre')\n",
    "tokenized_test_data = tokenizer.texts_to_sequences(test_data_text)\n",
    "processed_test_data = pad_sequences(tokenized_test_data,maxlen=pad_length, padding='pre')\n",
    "\n",
    "train_target = np.array(train_data_label)\n",
    "test_target = np.array(test_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** I originally had tried `padding='post'` which led to bad results\n",
    "* having a bunch of 0s at the end of a sequence is really bad when you are only sending the last output to the next layer\n",
    "* in general, we shouldn't be using post-padding with recurrent networks\n",
    " - unfortunately, this doesn't seem to be the problem with our encoder-decoder example, but it is still worth going back and fixing if you want to keep working with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple LSTM-based architecture\n",
    "\n",
    "Since this is a binary classification problem, we can use a sigmoid activation and `binary_crossentropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=pad_length))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(hidden_layer_size))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 50)           500000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 560501 (2.14 MB)\n",
      "Trainable params: 560501 (2.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 162s 412ms/step - loss: 0.4662 - accuracy: 0.7711 - val_loss: 0.3616 - val_accuracy: 0.8500\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 158s 403ms/step - loss: 0.2470 - accuracy: 0.9062 - val_loss: 0.3058 - val_accuracy: 0.8739\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 158s 405ms/step - loss: 0.1915 - accuracy: 0.9306 - val_loss: 0.3229 - val_accuracy: 0.8660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d7ca6c80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(processed_train_data,\n",
    "          train_target,\n",
    "          epochs = 3,\n",
    "          batch_size = 64,\n",
    "          validation_data=(processed_test_data,test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "The source I got this code from (https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ ) included Dropout layers - you can try uncommenting them and see what it does.\n",
    "\n",
    "It's also equivalent to writing\n",
    "\n",
    "`model.add(LSTM(hidden_layer_size), dropout=0.2, recurrent_dropout=0.2)`\n",
    "\n",
    "Do some searching and see what you can find out about what dropout layers do and why people use them. Discuss your findings with your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Run an experiment: What is the difference between using `SimpleRNN` and `LSTM` with this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Do one of the following:\n",
    "\n",
    "1. Redo your experiment with another classification dataset. Choose something with more than 2 classes - this will be good practice is understanding the difference you need to make to the model and data prep. Describe your data and results as usual.\n",
    "    * I also suggest including a GRU layer in your experiment as well: https://keras.io/api/layers/recurrent_layers/gru/\n",
    "\n",
    "2. Edit the Encoder-Decoder code from last time to use LSTM or GRU.\n",
    "    * Note that since LSTM returns both a context and hidden state, you will get an output, a hidden state, and context returned from the LSTM layer (instead of just the output and state). It will look something like\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_rnn = LSTM(100, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_rnn(enc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you will pass both state_h, state_c as the *context vector* which is the initial state for the decoder. See the source from last time to flesh out the example: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
