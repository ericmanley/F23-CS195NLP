{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Embeddings\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F5_4_Embeddings.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Word2Vec Tutorial - The Skip-Gram Model by Chris McCormick: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "Word2Vec - Negative Sampling made easy by Munesh Lakhey: https://medium.com/@mnshonco/word2vec-negative-sampling-made-easy-9a587cb4695f\n",
    "\n",
    "Keras Embedding Layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "Keras Tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset for today\n",
    "\n",
    "AG News dataset\n",
    "* short news articles\n",
    "* four classes: World, Sports, Business, Sci/Tech\n",
    "\n",
    "https://huggingface.co/datasets/ag_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "print(data[\"train\"][\"text\"][0])\n",
    "\n",
    "# 0 is World\n",
    "# 1 is Sports\n",
    "# 2 is Business\n",
    "# 3 is Sci/Tech\n",
    "print(data[\"train\"][\"label\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical classification example (with > 2 classes) \n",
    "\n",
    "We've only seen binary classification examples so far\n",
    "* used binary_crossentropy loss and a sigmoid output unit\n",
    "\n",
    "For more than two classes, use categorical_crossentropy and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of a sentence from the dataset: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Here's an example of a tokenized sentence converted into a sequence of integers: [442, 441, 1681, 14528, 108, 64, 1, 850, 21, 21, 753, 8196, 442, 6640, 10231, 2927, 4, 5810, 25989, 40, 4049, 797, 332]\n",
      "Here's an example after it has been padded: [442, 441, 1681, 14528, 108, 64, 1, 850, 21, 21, 753, 8196, 442, 6640, 10231, 2927, 4, 5810, 25989, 40, 4049, 797, 332]\n",
      "Here's an example of what the target label looks like: [0. 0. 1. 0.]\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 2s 562us/step - loss: 139.1792 - accuracy: 0.2525\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 2s 546us/step - loss: 1.6659 - accuracy: 0.2538\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 2s 505us/step - loss: 1.3985 - accuracy: 0.2507\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 2s 517us/step - loss: 1.3910 - accuracy: 0.2482\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 2s 496us/step - loss: 1.3885 - accuracy: 0.2486\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 2s 518us/step - loss: 1.3863 - accuracy: 0.2509\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 2s 501us/step - loss: 1.3872 - accuracy: 0.2486\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 2s 485us/step - loss: 1.3861 - accuracy: 0.2513\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 2s 488us/step - loss: 1.3861 - accuracy: 0.2473\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 2s 490us/step - loss: 1.3864 - accuracy: 0.2489\n",
      "238/238 [==============================] - 0s 431us/step - loss: 1.4060 - accuracy: 0.2503\n",
      "Test accuracy: 25.03%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "print(\"Here's an example of a sentence from the dataset:\",data[\"train\"][\"text\"][0])\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
    "print(\"Here's an example of a tokenized sentence converted into a sequence of integers:\",train_sequences[0])\n",
    "\n",
    "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
    "max_length = 100  # This should be adjusted based on the dataset\n",
    "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "print(\"Here's an example after it has been padded:\",train_sequences[0])\n",
    "\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
    "print(\"Here's an example of what the target label looks like:\", train_labels_array[0])\n",
    "\n",
    "\n",
    "#create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why does this do so badly?\n",
    "\n",
    "No matter how many epochs you run this for, it will not get any better\n",
    "\n",
    "**The problem:** the integer encoding\n",
    "\n",
    "So what do we do if we still want to feed *sequential* data - we don't just want a bag of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Don't just represent words with a single number - use a whole vector\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/embeddings.png\"> \n",
    "</div>\n",
    "\n",
    "image source: https://stackoverflow.com/questions/46155868/keras-embedding-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in reality, you can't label the dimensions with exact meanings like \"living being\", \"feline\", etc.\n",
    "* but you don't need to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we come up with these embeddings?\n",
    "\n",
    "First, we'll come up with a \"fake\" learning problem and then extract information about words from the model\n",
    "\n",
    "**fake learning problem** skip-grams: predict the words that appear around a given word\n",
    "\n",
    "We can pick any window size - this one uses size 2 (2 words before and 2 after)\n",
    "\n",
    "\n",
    "<div>\n",
    "   <img src=\"images/skip_gram_problem.png\"> \n",
    "</div>\n",
    "\n",
    "image source: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Before we can create a model for this, we need an initial numerical encoding of words\n",
    "\n",
    "**One Hot Encoding** uses a vector with length equal to the size of the vocabulary\n",
    "* 1 in the spot representing that word\n",
    "* 0 in all others\n",
    "\n",
    "|            | the | quick | brown | fox | jumps | over | lazy | dog |\n",
    "|------------|-----|-----|-----|----|-----|-----|-----|-----|\n",
    "| \"fox\" | 0   | 0   | 0   | 1  | 0   | 0   | 0   | 0   |\n",
    "| \"dog\" | 0 | 0   | 0   | 0   | 0  | 0   | 0   | 1   | \n",
    "\n",
    "We can use Keras' `to_categorical` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#put a 1 at index 3 of 8\n",
    "print( to_categorical(3, num_classes=8)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's get started\n",
    "\n",
    "Here are some toy sentences we can work with.\n",
    "\n",
    "We'll use Keras' tokenizer and show the mapping of words to indexes it cam up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dogs': 1, 'i': 2, 'that': 3, 'cats': 4, 'the': 5, 'animal': 6, 'have': 7, 'heard': 8, 'can': 9, 'be': 10, 'adopted': 11, 'some': 12, 'from': 13, 'shelter': 14, \"don't\": 15, 'you': 16, 'know': 17, 'and': 18, 'both': 19, 'like': 20, 'scritches': 21, 'are': 22, 'or': 23, 'your': 24, 'favorite': 25, 'obedient': 26, 'independent': 27, 'sharks': 28, 'live': 29, 'in': 30, 'ocean': 31, 'many': 32, 'birds': 33, 'fly': 34, 'to': 35, 'get': 36, 'around': 37}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Sample data\n",
    "sentences = [\n",
    "    \"I adopted some dogs from the animal shelter\",\n",
    "    \"don't you know that dogs and cats both like scritches\",\n",
    "    \"are cats or dogs your favorite animal\",\n",
    "    \"I have heard that dogs can be obedient\",\n",
    "    \"I have heard that cats can be independent\",\n",
    "    \"sharks live in the ocean\",\n",
    "    \"many birds fly to get around\"\n",
    "]\n",
    "\n",
    "# Tokenize and create vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### So now we know all the words in our vocabulary\n",
    "\n",
    "0 will be a special index to represent \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1 #we also have to account for 0\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Now we tokenize the sentences and convert them to their indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 11, 12, 1, 13, 5, 6, 14], [15, 16, 17, 3, 1, 18, 4, 19, 20, 21], [22, 4, 23, 1, 24, 25, 6], [2, 7, 8, 3, 1, 9, 10, 26], [2, 7, 8, 3, 4, 9, 10, 27], [28, 29, 30, 5, 31], [32, 33, 34, 35, 36, 37]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Creating the skip grams\n",
    "\n",
    "Keras has a nice function for this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[13, 14], [5, 19], [13, 19], [1, 29], [5, 14], [11, 14], [11, 13], [11, 19], [12, 5], [5, 6], [2, 9], [12, 10], [1, 11], [6, 5], [1, 13], [13, 5], [5, 1], [1, 14], [13, 36], [5, 13], [5, 5], [11, 2], [5, 14], [12, 28], [13, 3], [14, 6], [14, 13], [6, 14], [13, 1], [6, 13], [2, 11], [2, 12], [13, 34], [2, 14], [12, 13], [14, 5], [11, 12], [5, 13], [6, 1], [14, 6], [14, 27], [12, 21], [11, 5], [12, 2], [11, 1], [5, 18], [1, 12], [1, 23], [6, 12], [14, 23], [12, 1], [11, 23], [12, 10], [5, 12], [1, 12], [1, 18], [12, 34], [2, 30], [6, 24], [12, 11], [1, 5], [2, 1], [6, 11], [13, 12], [1, 2], [1, 5], [13, 22], [13, 11], [13, 6], [1, 6], [13, 5], [6, 29]], [1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "sequence_skipgrams = skipgrams(sequences[0],vocabulary_size=vocabulary_size,window_size=3)\n",
    "print(sequence_skipgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Group Discussion\n",
    "\n",
    "What does this output mean?\n",
    "\n",
    "What are the different pairs of numbers?\n",
    "\n",
    "What are all the 0s and 1s at the end?\n",
    "\n",
    "The `skipgrams` function is doing something called **negative sampling**. Write your guess for what that means here.\n",
    "\n",
    "**Negative sampling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Group Exercise: Prepare data for the skip-gram model\n",
    "\n",
    "use `sequence_skipgrams` to prepare the data\n",
    "\n",
    "Each training example should be the one hot encoded word concatenated with the one hot encoded context word\n",
    "\n",
    "Example: if \"brown\" is index 3 and \"fox\" is index 4 (and vocab size is 8), then we have\n",
    "\n",
    "\"brown\":`[0,0,0,1,0,0,0,0]`\n",
    "\n",
    "\"fox\":`[0,0,0,0,1,0,0,0]`\n",
    "\n",
    "training example input: `[0,0,0,1,0,0,0,0, 0,0,0,0,1,0,0,0]`\n",
    "\n",
    "*Hint:* you will need to use the `to_categorical` function\n",
    "\n",
    "Then, do it for all the skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple model\n",
    "\n",
    "Let's draw what this model looks like on the white board\n",
    "\n",
    "Then we'll train it with the data we prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2aa893220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Model\n",
    "embedding_model = Sequential()\n",
    "# we input a one-hot vector for the word and its context word, so vocabulary_size*2\n",
    "embedding_model.add(Dense(50, input_dim=vocabulary_size*2, activation='relu')) \n",
    "embedding_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "embedding_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "embedding_model.fit(inputs_array, labels_array, epochs=5000, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where is the word embedding in all this?\n",
    "\n",
    "The weights going from the word's index to hidden layer nodes represent what the model learned about this word, so we'll use those weights as the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: dogs\n",
      "index: 1\n",
      "The embedding for the word 'dogs' is: [-0.05164365  0.15345961  0.27586758  0.03484106  0.01857457  0.12105501\n",
      " -0.12195754  0.3844508   0.52245075 -0.35440248 -1.3468415  -0.35215762\n",
      " -0.18128224  0.342001    0.29873767 -0.80867183  0.37532702 -0.06673234\n",
      "  0.36035883  0.27477598  0.04212696  0.46738985 -0.17842224 -0.08797641\n",
      " -0.12868786  0.07381967 -0.10810392  0.09558013 -0.13766237 -0.35415867\n",
      "  0.02245924  0.21492355  0.39072448  0.2834317  -0.28319678 -0.35475025\n",
      "  0.21243913 -0.47210756  0.27767736  0.27440387  0.04292269 -0.08132959\n",
      " -0.04046155  0.0301704  -0.25367644 -0.05824045 -1.1348718  -0.4878483\n",
      "  0.13669956  0.25040466]\n"
     ]
    }
   ],
   "source": [
    "word = \"dogs\"\n",
    "print(\"Word:\", word)\n",
    "word_index = tokenizer.word_index[word]\n",
    "print(\"index:\",word_index)\n",
    "weights = embedding_model.layers[0].get_weights()[0]\n",
    "word_embedding = weights[word_index]\n",
    "print(f\"The embedding for the word '{word}' is: {word_embedding.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We could make this into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02375348  0.11949711  0.06044994 -0.03714155 -0.19955637 -0.07614389\n",
      " -0.05473538  0.02902077  0.18962364 -0.21753238  0.16662757  0.10069387\n",
      "  0.17979206 -0.16885345  0.01473857 -0.0690915   0.13704668  0.06257458\n",
      "  0.17304529  0.10747083 -0.026731   -0.05845037 -0.16764595  0.02230105\n",
      " -0.17020188 -0.08070949  0.03694488  0.21064727 -0.14492504  0.14048947\n",
      " -0.21059206  0.10628273 -0.05432342 -0.00303474 -0.14579701  0.01792982\n",
      " -0.03013816  0.04185806  0.20681618 -0.14459702  0.0063161  -0.1687189\n",
      " -0.0495749   0.0276676   0.1956714  -0.05057637 -0.10117345  0.03816085\n",
      "  0.18405418  0.10212852]\n",
      "[-0.05164365  0.15345961  0.27586758  0.03484106  0.01857457  0.12105501\n",
      " -0.12195754  0.3844508   0.52245075 -0.35440248 -1.3468415  -0.35215762\n",
      " -0.18128224  0.342001    0.29873767 -0.80867183  0.37532702 -0.06673234\n",
      "  0.36035883  0.27477598  0.04212696  0.46738985 -0.17842224 -0.08797641\n",
      " -0.12868786  0.07381967 -0.10810392  0.09558013 -0.13766237 -0.35415867\n",
      "  0.02245924  0.21492355  0.39072448  0.2834317  -0.28319678 -0.35475025\n",
      "  0.21243913 -0.47210756  0.27767736  0.27440387  0.04292269 -0.08132959\n",
      " -0.04046155  0.0301704  -0.25367644 -0.05824045 -1.1348718  -0.4878483\n",
      "  0.13669956  0.25040466]\n",
      "7.2377887\n",
      "8.325669\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word,embedding_model):\n",
    "    word_index = tokenizer.word_index[word]\n",
    "    weights = embedding_model.layers[0].get_weights()[0]\n",
    "    word_embedding = weights[word_index]\n",
    "    return word_embedding\n",
    "\n",
    "cats_embedding = get_embedding(\"cats\",embedding_model)\n",
    "dogs_embedding = get_embedding(\"dogs\",embedding_model)\n",
    "shelter_embedding = get_embedding(\"shelter\",embedding_model)\n",
    "sharks_embedding = get_embedding(\"sharks\",embedding_model)\n",
    "print(cats_embedding)\n",
    "print(dogs_embedding)\n",
    "\n",
    "print( np.sum(np.square(cats_embedding-dogs_embedding)) )\n",
    "print( np.sum(np.square(sharks_embedding-shelter_embedding)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Create word embeddings for the AG News dataset.\n",
    "\n",
    "Put all of the code into one cell so it isn't spread all throughout the notebook.\n",
    "\n",
    "Show some example word embeddings.\n",
    "\n",
    "Describe your results and reflect on them\n",
    "* How many unique words does the dataset have?\n",
    "* How many training epochs do you think are appropriate? Why?\n",
    "* How could you go about figuring out if these embeddings are useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras Embedding Layer\n",
    "\n",
    "Keras provides an `Embedding` layer that you can put at the beginning of your network which allows you to learn the embeddings as part of the main training process.\n",
    "\n",
    "Let's see how it does when we include this in our example from the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 0.3233 - accuracy: 0.8827\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 0.1125 - accuracy: 0.9632\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0313 - accuracy: 0.9914\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0175 - accuracy: 0.9963\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 0.0123 - accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0074 - accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 28s 8ms/step - loss: 0.0062 - accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.0043 - accuracy: 0.9983\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 29s 8ms/step - loss: 0.0034 - accuracy: 0.9987\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 28s 8ms/step - loss: 0.0036 - accuracy: 0.9986\n",
      "238/238 [==============================] - 0s 628us/step - loss: 0.5424 - accuracy: 0.8984\n",
      "Test accuracy: 89.84%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = load_dataset(\"ag_news\")\n",
    "\n",
    "# Prepare the tokenizer and fit on the training text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"train\"][\"text\"])\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(data[\"train\"][\"text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(data[\"test\"][\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "# Pad sequences to ensure uniform length; you can decide the max length based on your dataset's characteristics\n",
    "max_length = 100  # This should be adjusted based on the dataset\n",
    "train_encoding_array = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_encoding_array = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "train_labels_array = to_categorical(train_labels, num_classes=4)\n",
    "test_labels_array = to_categorical(test_labels, num_classes=4)\n",
    "\n",
    "#create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, input_dim=max_length, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_encoding_array, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_encoding_array, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
