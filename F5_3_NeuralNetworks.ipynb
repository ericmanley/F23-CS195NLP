{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F5_3_NeuralNetworks.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "SLP: Neural Networks and Neural Language Models, Chapter 7 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/7.pdf\n",
    "\n",
    "Artificial Neural Networks, Chapter 4 of Machine Learning by Tom M. Mitchell http://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf\n",
    "\n",
    "Sequential Model from Keras Developer Guide: https://keras.io/guides/sequential_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.4)\n",
      "Requirement already satisfied: sklearn in /Users/000794593/Library/Python/3.10/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: keras in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow in /Users/000794593/Library/Python/3.10/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: transformers in /Users/000794593/Library/Python/3.10/lib/python/site-packages (4.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from sklearn) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorflow-macos==2.14.0->tensorflow) (63.4.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: filelock in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/000794593/Library/Python/3.10/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets sklearn keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Integer Encoding\n",
    "\n",
    "We tried machine learning with text where each word was assigned a number - integer encoding\n",
    "\n",
    "We have to make sure each input has the same size. Since text inputs are different sizes\n",
    "* pad small ones with zeros\n",
    "* truncate long ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726605504587156\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "\n",
    "def prepare_text_data(text_list,tokenizer,encoded_length=512):\n",
    "    encodings = []\n",
    "    for curr_example in text_list:\n",
    "        curr_tokens = tokenizer.tokenize(curr_example)\n",
    "        curr_encodings = tokenizer.convert_tokens_to_ids(curr_tokens)\n",
    "\n",
    "        # truncate sequences that are too long\n",
    "        if len(curr_encodings) > encoded_length:\n",
    "            curr_encodings = curr_encodings[:encoded_length]\n",
    "        # pad sequences that are too short with 0s\n",
    "        elif len(curr_encodings) < encoded_length:\n",
    "            curr_encodings = curr_encodings + [0]*(encoded_length-len(curr_encodings))\n",
    "\n",
    "        encodings.append(curr_encodings)\n",
    "        \n",
    "    return encodings\n",
    "\n",
    "\n",
    "\n",
    "train_encoding = prepare_text_data(data[\"train\"][\"text\"],tokenizer)\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_encoding = prepare_text_data(data[\"test\"][\"text\"],tokenizer)\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_encoding,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_encoding)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Bag-of-Words Encoding\n",
    "\n",
    "Choose vocabulary (say 5000 most common words) one column for each word\n",
    "\n",
    "row contains counts for each word\n",
    "\n",
    "**Example**\n",
    "\n",
    "*Sentence 1:* \"The cat sat on the hat\"\n",
    "\n",
    "*Sentence 2:* \"The dog ate the cat and the hat\" \n",
    "\n",
    "*Vocabulary:* { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "\n",
    "|            | the | cat | sat | on | hat | dog | ate | and |\n",
    "|------------|-----|-----|-----|----|-----|-----|-----|-----|\n",
    "| Sentence 1 | 2   | 1   | 1   | 1  | 1   | 0   | 0   | 0   |\n",
    "| Sentence 2 | 3   | 1   | 0   | 0  | 1   | 1   | 1   | 1   |\n",
    "\n",
    "\n",
    "**The downside:** this doesn't maintain any information about word order - thus the \"bag\" of words\n",
    "\n",
    "`scikit-learn` provides a Bag-of-Words encoder called `CountVectorizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "# remove stop words\n",
    "vectorizer = CountVectorizer(max_features=5000,stop_words=\"english\")  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TD-IDF Encoding\n",
    "\n",
    "**TF-IDF:** Term Frequency - Inverse Document Frequency\n",
    "\n",
    "**Term Frequency:** How often does the word appear in the example, like CountVectorizer\n",
    "* actually take the $\\log$ of it\n",
    "\n",
    "**Document Frequency:** What fraction of the *documents* (or *training-examples*) does the word appear in?\n",
    "\n",
    "**Inverse Document Frequency:** (number of documents) / (number of documents containing the word)\n",
    "* if a word is in only a few documents, you get a big number\n",
    "* if a word appears in lots of documents, you get a small number\n",
    "\n",
    "When encoding a new example, multiply the Term Frequency of the word in this example by the Inverse Document Frequency of the training set\n",
    "* gives higher weight to words that are differentiators\n",
    "* stop words should automatically be de-emphasized\n",
    "\n",
    "**Example:**\n",
    "Document collection: all of Shakespeare's plays\n",
    "\n",
    "The word `Romeo` appears 113 times but only in 1 document\n",
    "\n",
    "The word `action` appears 113 time but in 31 documents \n",
    "\n",
    "so Romeo will get a much higher weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952293577981651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(train_vectors,train_labels)\n",
    "\n",
    "predictions = lr_model.predict(test_vectors)\n",
    "\n",
    "print( accuracy_score(test_labels,predictions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "Hopefully neural networks are familiar to you from your Machine Learning course - but here is a review of some important aspects\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/fullyconnected.png\">\n",
    "</div>\n",
    "\n",
    "image credit: http://neuralnetworksanddeeplearning.com/chap6.html\n",
    "\n",
    "For NLP, vectors representing words/sequences-of-words are the input layer\n",
    "\n",
    "Output layer: the class for text classification, the next word in the sequence, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Nodes\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/ann-perceptron.png\">\n",
    "</div>\n",
    "\n",
    "image credit: Machine Learning by Tom M. Mitchell, Chapter 4, http://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "The basic perceptron *squashing function* just calls anything positive a 1 and anything negative a 0, but modern neural networks use many other activation functions.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/activation_binary_step.png\" width=300>\n",
    "</div>\n",
    "\n",
    "Activation Function images from https://en.wikipedia.org/wiki/Activation_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$\\sigma (x) = {\\frac {1}{1+e^{-x}}}$\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/activation_logistic.png\" width=300>\n",
    "</div>\n",
    "\n",
    "differentiable, so the calculus in the training algorithm works out\n",
    "\n",
    "often used in an output layer where you have a binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperbolic Tangent Function\n",
    "\n",
    "$\\tanh(x) = {\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}$\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/activation_tanh.png\" width=300>\n",
    "</div>\n",
    "\n",
    "like sigmoid, but approximates identity near origin - learns efficiently with small, random, initial weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identity Function\n",
    "\n",
    "$f(x) = x$\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/activation_identity.png\" width=300>\n",
    "</div>\n",
    "\n",
    "take the output from the node as is - helpful if your output is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rectified-Linear Unit - ReLU\n",
    "\n",
    "$f(x) = \\mbox{max}(0,x)$\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/activation_relu.png\" width=300>\n",
    "</div>\n",
    "\n",
    "either \"doesn't fire\" or \"fires with measurable intensity\" - biologically motivated\n",
    "\n",
    "often used in hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Softmax\n",
    "\n",
    "Used for outpus layer when you have more than two possible classes - like if you are predicting the next word\n",
    "\n",
    "Like arg-max (which argument results in the maximum value)\n",
    "\n",
    "Which class has the largest output value?\n",
    "\n",
    "However, it is *soft* in that it really applies a probability to every possible value, weighted heavily to the one with the largest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "1. Start with random weights or weights learned from some other related task\n",
    "2. Feed a training example into the network and get a prediction\n",
    "3. Calculate the **loss function** a measurement of how far away the prediction was from the target value\n",
    "4. Adjust weights in an attempt to reduce the loss (Calculus)\n",
    "    * derivative of the loss function with respect to the weights of the network\n",
    "    * start with the output layer and move towards the front of the network (backpropagation)\n",
    "    * adjustments for middle layers based on adjustment of later layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "For numerical outputs, use mean-squared-error\n",
    "\n",
    "For binary/categorical use cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{true}$: the actual target value\n",
    "\n",
    "$y_{pred}$: the predicted target value\n",
    "\n",
    "crossentropy_loss = $-( y_{true}\\log(y_{pred}) + (1-y_{true})\\log(1-y_{pred} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5.5 of SLP (https://web.stanford.edu/~jurafsky/slp3/5.pdf) shows how we get this function.\n",
    "\n",
    "Intuitively, imagine that we have a binary output layer - it should always be 0 or 1.\n",
    "\n",
    "Let's say $y_{true}$ is 1 and our model predicts 0.9 (pretty confident it's a \"1\", but the final activation layer allows it to float a little\n",
    "\n",
    "Then the crossentropy loss is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "y_true = 1\n",
    "y_pred = 0.9\n",
    "\n",
    "ce_loss = -(y_true*math.log(y_pred) + (1-y_true)*math.log(y_pred))\n",
    "print(ce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That's a small loss/error\n",
    "\n",
    "compare with $y_{pred}$ of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "y_true = 1\n",
    "y_pred = 0.1\n",
    "\n",
    "ce_loss = -(y_true*math.log(y_pred) + (1-y_true)*math.log(y_pred))\n",
    "print(ce_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Packages\n",
    "\n",
    "PyTorch (initially developed by Meta AI)\n",
    "\n",
    "Tensorflow (initially developed by Google)\n",
    "\n",
    "Keras - easy to use Python interface for TensorFlow\n",
    "* support for PyTorch coming soon\n",
    "\n",
    "all are free and open-source\n",
    "\n",
    "We'll start with Keras, but we may later use TensorFlow and/or PyTorch directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing Data for Keras\n",
    "\n",
    "Keras (and other neural network packages) requires data to be in `numpy` arrays (the main package for fast, numerical arrays/vectors/matrices in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Deysi/spam-detection-dataset\")\n",
    "\n",
    "train_texts = data[\"train\"][\"text\"]\n",
    "train_labels = data[\"train\"][\"label\"]\n",
    "test_texts = data[\"test\"][\"text\"]\n",
    "test_labels = data[\"test\"][\"label\"]\n",
    "\n",
    "# Consider top 5000 frequent words\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_vectors = vectorizer.transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# the sklearn vectors need to be converted to numpy arrays for this library\n",
    "train_vectors_arrays = train_vectors.toarray()\n",
    "test_vectors_arrays = test_vectors.toarray()\n",
    "\n",
    "#convert labels from spam/not-spam into 1/0\n",
    "train_labels_binary = []\n",
    "for label in train_labels:\n",
    "    if label == \"spam\":\n",
    "        train_labels_binary.append(1)\n",
    "    else:\n",
    "        train_labels_binary.append(0)\n",
    "         \n",
    "#convert labels from spam/not-spam into 1/0\n",
    "test_labels_binary = []\n",
    "for label in test_labels:\n",
    "    if label == \"spam\":\n",
    "        test_labels_binary.append(1)\n",
    "    else:\n",
    "        test_labels_binary.append(0)        \n",
    "        \n",
    "# Convert values into arrays\n",
    "train_labels_array = np.array(train_labels_binary)\n",
    "test_labels_array = np.array(test_labels_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Defining the model architecture\n",
    "\n",
    "A *Sequential* model allows you to define a neural network structure one layer at a time.\n",
    "\n",
    "The first layer has 5000 inputs because our text vectors contain 5000 features\n",
    "\n",
    "We define the first two layers to have 10 nodes each - these are parameters that can be experimented with\n",
    "\n",
    "The output layer has 1 sigmoid node because there are only two possible outputs (for categorical, use `'softmax'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#create a neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=5000, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Training the model\n",
    "\n",
    "You need to define the loss function and optimizer algorithm\n",
    "\n",
    "for more than 2 categories, use `\"categorical_crossentropy\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 1s 755us/step - loss: 0.3931 - accuracy: 0.9774\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 728us/step - loss: 0.0308 - accuracy: 0.9990\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 711us/step - loss: 0.0071 - accuracy: 0.9993\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 706us/step - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 698us/step - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 734us/step - loss: 0.0014 - accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 714us/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 709us/step - loss: 8.2582e-04 - accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 734us/step - loss: 6.7834e-04 - accuracy: 0.9998\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 740us/step - loss: 5.6080e-04 - accuracy: 0.9998\n",
      "86/86 [==============================] - 0s 527us/step - loss: 0.0043 - accuracy: 0.9982\n",
      "Test accuracy: 99.82%\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vectors_arrays, train_labels_array, epochs=10, verbose=1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_vectors_arrays, test_labels_array)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group Exercise\n",
    "\n",
    "Even though we have established that *integer encoding* is a bad way to encode text, we will eventually want feed in encodings that represent one word at a time in a sequence - unlike BoW and TD-IDF which aggregate all words in the text into a single vector, so let's practice setting it up with integer encoding.\n",
    "\n",
    "Make the Keras neural network work with the Integer Encoding approach we used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Select another Hugging Face dataset for text classification and get it working with the Keras neural network.\n",
    "\n",
    "Experiment with different numbers of layers and numbers of nodes in each layer. Record your results.\n",
    "\n",
    "Give a short write-up on the following\n",
    "* Describe your dataset, including the distribution of the target variable\n",
    "* Describe the results of the machine learning experiment\n",
    "* Interpret the results - How did this dataset compare with the spam dataset? Why do you think you got the results that you did?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
